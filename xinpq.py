import requests
from bs4 import BeautifulSoup
import re
import time

def get_max_page_number(base_url, headers):
    """
    æ ¹æ®æä¾›çš„ HTML ç»“æ„ç²¾ç¡®è·å–æœ€å¤§é¡µç 
    é€»è¾‘ï¼šåœ¨ class='pager' çš„å®¹å™¨ä¸­éå† <a> æ ‡ç­¾ï¼Œå¯»æ‰¾åŒ…å«â€œå°¾é¡µâ€æ–‡å­—çš„é“¾æ¥ï¼Œ
    å¹¶ä»ä¸­æå– index_(d+).html é‡Œçš„æ•°å­—ã€‚
    """
    try:
        response = requests.get(base_url, headers=headers, timeout=15)
        response.raise_for_status()
        # ä½¿ç”¨ lxml æé«˜è§£ææ•ˆç‡ï¼Œå¦‚æœªå®‰è£…è¯·æ”¹ä¸º 'html.parser'
        soup = BeautifulSoup(response.text, 'lxml')
        
        # 1. ä¼˜å…ˆåœ¨ pager å®¹å™¨å†…æŸ¥æ‰¾
        pager = soup.find('div', class_='pager')
        links = pager.find_all('a') if pager else soup.find_all('a')
        
        for link in links:
            text = link.get_text(strip=True)
            if 'å°¾é¡µ' in text:
                href = link.get('href', '')
                # æ­£åˆ™åŒ¹é… index_æ•°å­—.html
                match = re.search(r'index_(\d+)\.html', href)
                if match:
                    return int(match.group(1))
                # å¤‡é€‰åŒ¹é…ï¼šå¦‚æœé“¾æ¥ä¸­åªæœ‰ index_æ•°å­— (ä¸å¸¦.html)
                match = re.search(r'index_(\d+)', href)
                if match:
                    return int(match.group(1))
        
        # 2. å¦‚æœæ²¡æ‰¾åˆ°â€œå°¾é¡µâ€ï¼Œå°è¯•æ‰¾æœ€åä¸€ä¸ªæ•°å­—é¡µç 
        # åœ¨ <div class="pager"> ä¸­æ‰¾æ•°å­—
        if pager:
            page_numbers = []
            for a in pager.find_all('a'):
                if a.text.isdigit():
                    page_numbers.append(int(a.text))
            if page_numbers:
                return max(page_numbers)

    except Exception as e:
        print(f"âŒ è·å–æœ€å¤§é¡µç å¤±è´¥: {type(e).__name__} - {str(e)[:50]}")
    
    return 1 # é»˜è®¤è¿”å›ç¬¬ä¸€é¡µ

# è¯·æ±‚å¤´é…ç½®
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
}

# åŸºç¡€URLé…ç½®
domain = "https://www.yszzq.com"
tag_path = "/tags/xmlcjjk/"
base_url = f"{domain}{tag_path}index.html"

# 1. è·å–æœ€å¤§é¡µç 
print(f"ğŸ” æ­£åœ¨ä» {base_url} åˆ†æåˆ†é¡µä¿¡æ¯...")
max_page = get_max_page_number(base_url, headers)
print(f"ğŸ“Š æ¢æµ‹åˆ°æœ€å¤§é¡µæ•°: {max_page}")

# 2. ç”Ÿæˆæ‰€æœ‰åˆ†é¡µ URL
# ç¬¬ä¸€é¡µä¸º index.htmlï¼Œåç»­ä¸º index_2.html, index_3.html ...
urls = [base_url]
if max_page > 1:
    urls += [f"{domain}{tag_path}index_{i}.html" for i in range(2, max_page + 1)]

print(f"ğŸ¯ å¾…å¤„ç† URL æ€»æ•°: {len(urls)}")

all_results = []

# 3. å¾ªç¯çˆ¬å–æ¯ä¸€é¡µ
for index, url in enumerate(urls):
    try:
        if index > 0:
            time.sleep(1.2)  # ç¤¼è²Œå»¶æ—¶
            
        print(f"[{index + 1}/{len(urls)}] æ­£åœ¨æŠ“å–: {url}")
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'lxml')
        
        # åŒ¹é…å…³é”®è¯ï¼šæ¥å£ã€åœ°å€ã€APIã€èµ„æºåº“ç­‰
        pattern = re.compile(r'æ¥å£|åœ°å€|API|èµ„æº|èµ„æºåº“|èµ„æºæ¥å£|èµ„æºç½‘|json[\\u4e00-\\u9fa5]*', re.UNICODE)
        
        # åœ¨ list_txt å®¹å™¨å†…å¯»æ‰¾ç¬¦åˆæ¡ä»¶çš„é“¾æ¥
        list_container = soup.find('div', class_='list_txt')
        target_elements = list_container.find_all(string=pattern) if list_container else soup.find_all(string=pattern)

        for element in target_elements:
            parent = element.find_parent('a')
            if not parent or 'href' not in parent.attrs:
                continue
                
            raw_href = parent['href']
            title = element.strip()
            
            # æ„å»ºå®Œæ•´çš„èµ„æº URL (æ’é™¤å¤–é“¾å¹²æ‰°)
            if raw_href.startswith(('http://', 'https://')):
                final_url = raw_href
            else:
                if not raw_href.startswith('/'):
                    raw_href = '/' + raw_href.lstrip('./')
                final_url = f"{domain}{raw_href}"

            # è¿‡æ»¤é€»è¾‘ï¼šåŒ…å«å…³é”®è¯ï¼Œä¸”æ’é™¤æ ‡é¢˜å¸¦ XML çš„é¡¹
            valid_keywords = ["é‡‡é›†æ¥å£", "èµ„æºåº“", "èµ„æºæ¥å£", "é‡‡é›†APIæ¥å£"]
            is_valid = any(kw in title for kw in valid_keywords) and "XML" not in title
            
            if is_valid:
                entry = f"{title},{final_url}"
                if entry not in all_results:
                    all_results.append(entry)
                    print(f"  âœ… å‘ç°: {title[:20]}...")
                
    except Exception as e:
        print(f"  âŒ å‡ºé”™ {url}: {type(e).__name__}")

# 4. ä¿å­˜ç»“æœ
with open('pq.txt', 'w', encoding='utf-8') as f:
    f.write('\n'.join(all_results))

print(f"\nğŸ¯ å¤„ç†å®Œæˆï¼å…±æå–åˆ° {len(all_results)} æ¡æ¥å£è®°å½•ï¼Œå·²ä¿å­˜è‡³ pq.txt")
