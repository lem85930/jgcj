import requests
from bs4 import BeautifulSoup
import re
import time

def get_max_page_number(base_url, headers):
    """
    è·å–æœ€å¤§é¡µç 
    é€»è¾‘ï¼šåœ¨ index.html ä¸­å¯»æ‰¾â€œå°¾é¡µâ€é“¾æ¥ï¼Œæå– index_æ•°å­—.html ä¸­çš„æ•°å­—
    """
    try:
        response = requests.get(base_url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'lxml')
        last_page_link = soup.find('a', string='å°¾é¡µ')
        if last_page_link and 'href' in last_page_link.attrs:
            href = last_page_link['href']
            # åŒ¹é… index_(d+).html æ ¼å¼
            match = re.search(r'index_(d+)', href)
            if match:
                return int(match.group(1))
    except Exception as e:
        print(f"âŒ è·å–æœ€å¤§é¡µç å¤±è´¥: {type(e).__name__} - {str(e)[:50]}")
    return 1

# è¯·æ±‚å¤´é…ç½®
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
}

# åŸºç¡€URLé…ç½® (ç›´æ¥è®¿é—®ä¸»ç«™)
domain = "https://www.yszzq.com"
tag_path = "/tags/xmlcjjk/"
base_url = f"{domain}{tag_path}index.html"

# è·å–æœ€å¤§é¡µç 
max_page = get_max_page_number(base_url, headers)
print(f"ğŸ“Š æ£€æµ‹åˆ°æœ€å¤§é¡µæ•°: {max_page}")

# è‡ªåŠ¨ç”Ÿæˆå¾…çˆ¬å–çš„URLåˆ—è¡¨
# ç¬¬ä¸€é¡µæ˜¯ index.htmlï¼Œåç»­æ˜¯ index_2.html, index_3.html ...
urls = [base_url]
if max_page > 1:
    urls += [f"{domain}{tag_path}index_{i}.html" for i in range(2, max_page + 1)]

print(f"ğŸ¯ å…±ç”Ÿæˆ {len(urls)} ä¸ªç›®æ ‡URL")

all_results = []

for index, url in enumerate(urls):
    try:
        if index > 0:
            time.sleep(1.2)  # ç¤¼è²Œæ€§å»¶æ—¶
            
        print(f"æ­£åœ¨çˆ¬å– ({index + 1}/{len(urls)}): {url}")
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'lxml')
        
        # å…³é”®è¯æ­£åˆ™åŒ¹é…
        pattern = re.compile(r' æ¥å£|åœ°å€|API|èµ„æº|èµ„æºåº“|èµ„æºæ¥å£|èµ„æºç½‘|json[\u4e00-\u9fa5]*', re.UNICODE)
        
        for element in soup.find_all(string=pattern):
            parent = element.find_parent('a')
            if not parent or 'href' not in parent.attrs:
                continue
                
            raw_href = parent['href']
            title = element.strip()
            
            # æ„å»ºå®Œæ•´URL
            if raw_href.startswith(('http://', 'https://')):
                final_url = raw_href
            else:
                if not raw_href.startswith('/'):
                    raw_href = '/' + raw_href.lstrip('./')
                final_url = f"{domain}{raw_href}"

            # æ•°æ®è¿‡æ»¤ä¸ä¿å­˜é€»è¾‘
            valid_keywords = ["é‡‡é›†æ¥å£", "èµ„æºåº“", "èµ„æºæ¥å£", "é‡‡é›†APIæ¥å£"]
            is_valid = any(kw in title for kw in valid_keywords) and "XML" not in title
            
            if is_valid:
                all_results.append(f"{title},{final_url}")
                print(f"âœ… å‘ç°æœ‰æ•ˆæ¥å£: {title[:15]}... -> {final_url[:50]}...")
                
    except Exception as e:
        print(f"âŒ è®¿é—®å‡ºé”™ {url}: {type(e).__name__} - {str(e)[:50]}")

# æœ€ç»ˆç»“æœæŒä¹…åŒ–
with open('pq.txt', 'w', encoding='utf-8') as f:
    f.write('\n'.join(all_results))

print(f"\nğŸ¯ æŠ“å–å®Œæˆï¼")
print(f"âœ… ç»“æœå·²ä¿å­˜è‡³ pq.txtï¼Œå…±è®¡ {len(all_results)} æ¡æœ‰æ•ˆè®°å½•ã€‚")
