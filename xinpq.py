import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urlunparse
import re
import time

# ä»£ç†ç½‘å…³è·¯å¾„å¸¸é‡ï¼ˆä¿ç•™ï¼Œç”¨äºåç»­æ‹¼æ¥å­é“¾æ¥ï¼‰
PROXY_PATH = "/wztz/https/www.yszzq.com"

def build_proxy_url(original_url):
    """é‡æ„URLæ„å»ºé€»è¾‘ï¼ˆä¿ç•™ï¼Œç”¨äºå¤„ç†é¡µé¢å†…çš„å­é“¾æ¥ï¼‰"""
    parsed = urlparse(original_url)
    if parsed.netloc == "www.yszzq.com":
        new_path = f"{PROXY_PATH}{parsed.path}"
        return urlunparse((
            parsed.scheme,
            "wztz.wokaotianshi.eu.org",
            new_path,
            parsed.params,
            parsed.query,
            parsed.fragment
        ))
    return original_url

def get_max_page_number(proxy_base_url):
    """è·å–æœ€å¤§é¡µç ï¼ˆæ ¹æ®å®é™…ç½‘é¡µæºç é‡æ„é€»è¾‘ï¼‰"""
    try:
        # ç›´æ¥è¯·æ±‚ä»£ç†åçš„URL
        response = requests.get(proxy_base_url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'lxml')
        
        # 1. å®šä½åˆ†é¡µå®¹å™¨ï¼ˆclass="pager"çš„divï¼‰
        pager_div = soup.find('div', class_='pager')
        if not pager_div:
            print("âš ï¸ æœªæ‰¾åˆ°åˆ†é¡µå®¹å™¨")
            return 0
        
        # 2. æ‰¾åˆ°å°¾é¡µé“¾æ¥
        last_page_link = pager_div.find('a', string='å°¾é¡µ')
        if last_page_link and 'href' in last_page_link.attrs:
            href = last_page_link['href']
            # åŒ¹é…å°¾é¡µé“¾æ¥ä¸­çš„é¡µç ï¼šindex_æ•°å­—.html
            match = re.search(r'index_(\d+)\.html', href)
            if match:
                max_page = int(match.group(1))
                print(f"âœ… ä»å°¾é¡µé“¾æ¥æå–åˆ°æœ€å¤§é¡µç ï¼š{max_page}")
                return max_page
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœå°¾é¡µé“¾æ¥æ‰¾ä¸åˆ°ï¼Œæå–æ‰€æœ‰åˆ†é¡µæ•°å­—ä¸­çš„æœ€å¤§å€¼
        page_numbers = []
        # åŒ¹é…æ‰€æœ‰åˆ†é¡µé“¾æ¥ä¸­çš„æ•°å­—
        page_links = pager_div.find_all('a', href=re.compile(r'index_(\d+)\.html'))
        for link in page_links:
            match = re.search(r'index_(\d+)\.html', link['href'])
            if match:
                page_numbers.append(int(match.group(1)))
        
        # åŒ¹é…åˆ†é¡µåŒºåŸŸå†…çš„çº¯æ•°å­—spanï¼ˆå½“å‰é¡µç ï¼‰
        current_page_span = pager_div.find('span', string=re.compile(r'^\d+$'))
        if current_page_span:
            page_numbers.append(int(current_page_span.text.strip()))
        
        if page_numbers:
            max_page = max(page_numbers)
            print(f"âœ… ä»åˆ†é¡µæ•°å­—æå–åˆ°æœ€å¤§é¡µç ï¼š{max_page}")
            return max_page
        
    except Exception as e:
        print(f"âŒ è·å–æœ€å¤§é¡µç å¤±è´¥: {type(e).__name__} - {str(e)[:50]}")
    return 0

# ä¼˜åŒ–è¯·æ±‚å¤´ï¼Œå¢åŠ æ›´å¤šå…¼å®¹æ€§é…ç½®
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'X-Forwarded-Proto': 'https',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive'
}

# ç›´æ¥ä½¿ç”¨ä»£ç†åçš„å®Œæ•´åŸºç¡€URL
base_url = "https://wztz.wokaotianshi.eu.org/wztz/https/www.yszzq.com/tags/xmlcjjk/index.html"

# è·å–æœ€å¤§é¡µç 
max_page = get_max_page_number(base_url)

# ç”ŸæˆURLåˆ—è¡¨ï¼šç¬¬ä¸€é¡µä¸ºåŸbase_urlï¼Œç¬¬äºŒé¡µå¼€å§‹ä¸ºindex_2.htmlã€index_3.html...
urls = [base_url]  # ç¬¬ä¸€é¡µ
# ä»2å¼€å§‹ç”Ÿæˆåç»­é¡µç ï¼ˆç¬¬äºŒé¡µæ˜¯index_2.htmlï¼‰
if max_page >= 2:
    urls += [
        base_url.replace('index.html', f'index_{i}.html') 
        for i in range(2, max_page + 1)
    ]

print(f"ğŸ¯ å…±ç”Ÿæˆ {len(urls)} ä¸ªURL: {urls}")

all_results = []

# ä¼˜åŒ–åŒ¹é…è§„åˆ™ï¼š1. å»æ‰å¼€å¤´å¤šä½™çš„ç©ºæ ¼ 2. å¢åŠ åŒ¹é…ç²¾åº¦ 3. ä¸åŒºåˆ†å¤§å°å†™
target_pattern = re.compile(r'é‡‡é›†æ¥å£|èµ„æºåº“|èµ„æºæ¥å£|é‡‡é›†APIæ¥å£', re.UNICODE | re.IGNORECASE)
# è¾…åŠ©åŒ¹é…ï¼šç”¨äºå…ˆç­›é€‰å¯èƒ½åŒ…å«ç›®æ ‡å†…å®¹çš„aæ ‡ç­¾
pre_filter_pattern = re.compile(r'æ¥å£|åœ°å€|API|èµ„æº|èµ„æºåº“|èµ„æºæ¥å£|èµ„æºç½‘|json', re.UNICODE | re.IGNORECASE)

for index, url in enumerate(urls):
    try:
        if index > 0:
            time.sleep(1.5)  # å¢åŠ å»¶è¿Ÿï¼Œé¿å…è¢«åçˆ¬
        print(f"\nğŸ” æ­£åœ¨å¤„ç†ç¬¬ {index+1} é¡µ: {url}")
        
        # ç›´æ¥è¯·æ±‚ä»£ç†URLï¼ˆæ— éœ€å†è½¬æ¢ï¼‰
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        # å¼ºåˆ¶è®¾ç½®ç¼–ç ä¸ºUTF-8ï¼Œé¿å…ä¸­æ–‡ä¹±ç å¯¼è‡´åŒ¹é…å¤±è´¥
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'lxml')
        
        # å…³é”®ä¿®æ”¹1ï¼šå…ˆæ‰¾åˆ°æ‰€æœ‰aæ ‡ç­¾ï¼ˆç›®æ ‡é“¾æ¥éƒ½åœ¨aæ ‡ç­¾é‡Œï¼‰ï¼Œå†ç­›é€‰åŒ…å«ç›®æ ‡å…³é”®è¯çš„
        all_a_tags = soup.find_all('a', href=True)  # åªæ‰¾æœ‰hrefå±æ€§çš„aæ ‡ç­¾
        print(f"ğŸ“Œ æœ¬é¡µæ‰¾åˆ° {len(all_a_tags)} ä¸ªå¸¦é“¾æ¥çš„aæ ‡ç­¾")
        
        matched_count = 0
        for a_tag in all_a_tags:
            # è·å–aæ ‡ç­¾çš„æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼ˆåŒ…æ‹¬å­èŠ‚ç‚¹ï¼‰å’Œtitleå±æ€§
            a_text = a_tag.get_text(strip=True)  # å»æ‰é¦–å°¾ç©ºæ ¼
            a_title = a_tag.get('title', '').strip()  # è·å–titleå±æ€§
            combined_text = f"{a_text} {a_title}"  # åˆå¹¶æ–‡æœ¬å’Œtitleï¼Œæ‰©å¤§åŒ¹é…èŒƒå›´
            
            # å…ˆè¿‡æ»¤ï¼šå¦‚æœä¸åŒ…å«åŸºç¡€å…³é”®è¯ï¼Œç›´æ¥è·³è¿‡
            if not pre_filter_pattern.search(combined_text):
                continue
            
            # æ ¸å¿ƒåŒ¹é…ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡å…³é”®è¯
            if target_pattern.search(combined_text):
                matched_count += 1
                raw_href = a_tag['href']
                # ç¡®å®šæœ€ç»ˆæ ‡é¢˜ï¼ˆä¼˜å…ˆç”¨titleï¼Œæ²¡æœ‰åˆ™ç”¨æ–‡æœ¬ï¼‰
                title = a_title if a_title else a_text
                
                # å¤„ç†é“¾æ¥ï¼Œè½¬æ¢ä¸ºä»£ç†URL
                if raw_href.startswith(('http://', 'https://')):
                    final_url = build_proxy_url(raw_href)
                else:
                    if not raw_href.startswith('/'):
                        raw_href = '/' + raw_href.lstrip('./')
                    final_url = f"https://wztz.wokaotianshi.eu.org{PROXY_PATH}{raw_href}"
                
                # è¿‡æ»¤æ‰åŒ…å«XMLçš„æ ‡é¢˜ï¼ˆå¯é€‰ï¼Œæ ¹æ®ä½ çš„éœ€æ±‚è°ƒæ•´ï¼‰
                if "XML" not in title:
                    result_item = f"{title},{final_url}"
                    all_results.append(result_item)
                    print(f"âœ… åŒ¹é…åˆ°æœ‰æ•ˆå†…å®¹: {title[:30]}... -> {final_url[:60]}...")
                else:
                    print(f"âš ï¸ åŒ¹é…åˆ°ä½†åŒ…å«XMLï¼Œå·²è¿‡æ»¤: {title[:30]}...")
        
        print(f"ğŸ“Š æœ¬é¡µåŒ¹é…åˆ° {matched_count} ä¸ªåŒ…å«ç›®æ ‡å…³é”®è¯çš„aæ ‡ç­¾")
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚ç¬¬ {index+1} é¡µå¤±è´¥: {type(e).__name__} - {str(e)[:50]}")
    except Exception as e:
        print(f"âŒ å¤„ç†ç¬¬ {index+1} é¡µå¼‚å¸¸: {type(e).__name__} - {str(e)[:50]}")

# å…³é”®ä¿®æ”¹2ï¼šç¡®ä¿æ— è®ºæ˜¯å¦æœ‰ç»“æœï¼Œéƒ½ç”Ÿæˆpq.txtæ–‡ä»¶
try:
    with open('pq.txt', 'w', encoding='utf-8') as f:
        f.write('\n'.join(all_results))
    print(f"\nğŸ¯ ç»“æœå·²ä¿å­˜åˆ° pq.txtï¼šå…± {len(all_results)} æ¡æœ‰æ•ˆè®°å½•")
    # å¦‚æœæ²¡æœ‰ç»“æœï¼Œæç¤ºå¯èƒ½çš„åŸå› 
    if len(all_results) == 0:
        print("âš ï¸ æœªç”Ÿæˆä»»ä½•æœ‰æ•ˆè®°å½•ï¼Œå¯èƒ½åŸå› ï¼š1. é¡µé¢æ— åŒ¹é…å†…å®¹ 2. åŒ¹é…è§„åˆ™éœ€è°ƒæ•´ 3. ç½‘ç»œ/ä»£ç†é—®é¢˜")
except Exception as e:
    print(f"âŒ å†™å…¥pq.txtå¤±è´¥: {type(e).__name__} - {str(e)[:50]}")
