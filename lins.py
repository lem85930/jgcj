import requests 
import re 
import os 
from urllib.parse  import urlparse 
from time import sleep 
 
# å¢å¼ºè¯·æ±‚å¤´é…ç½® 
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'Accept-Encoding': 'gzip, deflate, br',
    'Accept-Language': 'zh-CN,zh;q=0.9',
    'Referer': 'https://www.yszzq.com/' 
}
 
TIMEOUT = 20 
MAX_RETRY = 3 
 
# ä¿®æ­£åçš„æ­£åˆ™è¡¨è¾¾å¼ 
URL_PATTERN = re.compile( 
    r'https?:\/\/(?:[a-zA-Z0-9\-\.]+\.)*yszzq\.com[\w\/\-\.%]+at\/xml(?:\?[\w=&]*)?',
    re.IGNORECASE 
)
 
def add_proxy_prefix(url):
    """åŠ¨æ€ä»£ç†è·¯å¾„è½¬æ¢"""
    parsed = urlparse(url)
    if 'yszzq.com'  in parsed.netloc: 
        new_path = f"/wztz/https/{parsed.netloc}{parsed.path}" 
        return parsed._replace(
            scheme="https",
            netloc="cfpgwztz.wofuck.rr.nu", 
            path=new_path 
        ).geturl()
    return url 
 
if not os.path.exists('pq.txt'): 
    raise FileNotFoundError("pq.txt  æ–‡ä»¶æœªæ‰¾åˆ°")
 
results = []
 
with open('pq.txt',  'r', encoding='utf-8') as file:
    lines = [line.strip() for line in file if line.strip()] 
 
for line in lines:
    try:
        title, url = line.split(',',  1)
        parsed = urlparse(url)
        
        if not all([parsed.scheme, parsed.netloc]): 
            print(f"ğŸš« æ— æ•ˆURL: {url}")
            continue 
 
        success = False 
        for retry in range(MAX_RETRY):
            try:
                resp = requests.get(url,  headers=HEADERS, 
                                  timeout=TIMEOUT + retry*3)
                
                # å¤„ç†ç‰¹æ®Šåçˆ¬æœºåˆ¶ 
                if resp.status_code  == 403:
                    print(f"â³ è§¦å‘åçˆ¬ [{url}] ç¬¬{retry+1}æ¬¡é‡è¯•...")
                    sleep(2 ** retry)
                    continue 
                
                resp.raise_for_status() 
                success = True 
                break 
                
            except requests.exceptions.RequestException  as e:
                print(f"âš ï¸ è¯·æ±‚å¼‚å¸¸: {type(e).__name__} - {str(e)[:50]}")
 
        if not success:
            continue 
 
        # å¢å¼ºå‹åŒ¹é… 
        if matches := URL_PATTERN.findall(resp.text): 
            for match in matches:
                final_url = add_proxy_prefix(match)
                results.append(f"{title},{final_url}") 
                print(f"âœ… åŒ¹é…æˆåŠŸ: {title[:15]}... -> {final_url[:60]}...")
 
    except Exception as e:
        print(f"âŒ å¤„ç†å¼‚å¸¸: {str(e)[:50]}")
 
if results:
    with open('maqu.txt',  'w', encoding='utf-8') as f:
        f.write('\n'.join(results)) 
    print(f"ğŸ¯ æˆåŠŸå†™å…¥ {len(results)} æ¡è®°å½•")
else:
    print("âš ï¸ æ— æœ‰æ•ˆæ•°æ®è¾“å‡ºï¼Œå»ºè®®æ£€æŸ¥ï¼š\n1. æºæ–‡ä»¶å†…å®¹æ ¼å¼\n2. ç½‘ç»œè¯·æ±‚æˆåŠŸç‡\n3. æ­£åˆ™åŒ¹é…æ¨¡å¼")
